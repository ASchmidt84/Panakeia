include "keys.conf"

play.application.loader = de.heilpraktikerelbmarsch.file.impl.loaders.PatientFileServiceLoader

#https://mkjwk.org/

pac4j.jwk = {"kty":"oct","k":${JWK_KEY},"alg":"HS512"}

pac4j.lagom.jwt.authenticator {
	signatures = [
		{
			algorithm = "HS512"
			jwk = ${?pac4j.jwk}
		}
	]
	encryptions = [
		{
			method = "A256CBC-HS512"
			algorithm = "dir"
			jwk = ${?pac4j.jwk}
		}
	]
}


db.default {
	driver = "org.postgresql.Driver"
	url = ${POSTGRESQL_URL}
	username = ${POSTGRESQL_USERNAME}
	password = ${POSTGRESQL_PASSWORD}
}

user.init {
	username = ${INIT_USERNAME}
	password = ${INIT_USERPASS}
}

jdbc-defaults.slick.profile = "slick.jdbc.PostgresProfile$"


akka.actor {
	serialization-bindings {
		# Commands won't use play-json but Akka's jackson support.
		# See https://doc.akka.io/docs/akka/2.6/serialization-jackson.html
//		"com.example.shoppingcart.impl.ShoppingCart$CommandSerializable" = jackson-json
		"de.heilpraktikerelbmarsch.file.impl.file.PatientFile$CommandSerializable" = jackson-json
	}
}


elasticsearch.config {
	seq = [{
		protocol = "http"
		host = "127.0.0.1"
		port = 9900
		prefix = ""
	}]
	username = "elastic"
	password = "elastic"
}

#akka.cluster.seed-nodes = [
#  "akka.tcp://application@10.233.74.116:2552",
#  "akka.tcp://application@10.233.75.3:2552",
#  "akka.tcp://application@10.233.97.173:2552"
#]
#akka.management.cluster.bootstrap.contactpoint.HttpClusterBootstrapRoutes [sourceThread=application-akka.actor.default-dispatcher-22, akkaTimestamp=11:30:58.409UTC, akkaSource=HttpClusterBootstrapRoutes(akka://application), sourceActorSystem=application] - Bootstrap request from 10.233.97.173:60526: Contact Point returning 3 seed-nodes ([Set(ClusterMember(akka.tcp://application@10.233.74.116:2552,-1895635541,Up,Set(dc-default)), ClusterMember(akka.tcp://application@10.233.75.3:2552,396825418,Up,Set(dc-default)), ClusterMember(akka.tcp://application@10.233.97.173:2552,1482037811,Up,Set(dc-default)))])



#lagom.broker.kafka.service-name = kafka # or whatever you have defined as the service name in k8s
#lagom.broker.kafka.service-name = '' # empty string disables service locator lookup
#lagom.broker.kafka.brokers = "kf1.mydomain.com:9092" # this can be a comma-separated string if you have >1

lagom.broker.kafka {
	# The name of the Kafka service to look up out of the service locator.
	# If this is an empty string, then a service locator lookup will not be done,
	# and the brokers configuration will be used instead.
	service-name = "kafka_native"
	service-name = ${?KAFKA_SERVICE_NAME}

	# The URLs of the Kafka brokers. Separate each URL with a comma.
	# This will be ignored if the service-name configuration is non empty.
	brokers = ${lagom.broker.defaults.kafka.brokers}
	#brokers = "kafka.avalon.svc.cluster.local:9092"

	client {
		default {
			# Exponential backoff for failures
			failure-exponential-backoff {
				# minimum (initial) duration until processor is started again
				# after failure
				min = 3s

				# the exponential back-off is capped to this duration
				max = 30s

				# additional random delay is based on this factor
				random-factor = 0.2
			}
		}

		# configuration used by the Lagom Kafka producer
		producer = ${lagom.broker.kafka.client.default}
		producer.role = ""

		# configuration used by the Lagom Kafka consumer
		consumer {
			failure-exponential-backoff = ${lagom.broker.kafka.client.default.failure-exponential-backoff}

			# The number of offsets that will be buffered to allow the consumer flow to
			# do its own buffering. This should be set to a number that is at least as
			# large as the maximum amount of buffering that the consumer flow will do,
			# if the consumer buffer buffers more than this, the offset buffer will
			# backpressure and cause the stream to stop.
			offset-buffer = 100

			# Number of messages batched together by the consumer before the related messages'
			# offsets are committed to Kafka.
			# By increasing the batching-size you are trading speed with the risk of having
			# to re-process a larger number of messages if a failure occurs.
			# The value provided must be strictly greater than zero.
			batching-size = 20

			# Interval of time waited by the consumer before the currently batched messages'
			# offsets are committed to Kafka.
			# This parameter is useful to ensure that messages' offsets are always committed
			# within a fixed amount of time.
			# The value provided must be strictly greater than zero.
			batching-interval = 5 seconds
		}
	}
}

lagom.services {
//	cas_native = "_cql._tcp.cassandra.svc.cluster.local"
	kafka_native = "_kafka._tcp.kafka-headless.svc.cluster.local"
}

# The properties below override Lagom default configuration with the recommended values for new projects.
#
# Lagom has not yet made these settings the defaults for backward-compatibility reasons.

# Prefer 'ddata' over 'persistence' to share cluster sharding state for new projects.
# See https://doc.akka.io/docs/akka/current/cluster-sharding.html#distributed-data-vs-persistence-mode
akka.cluster.sharding.state-store-mode = ddata

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
	"akka.Done"                 = akka-misc
	"akka.actor.Address"        = akka-misc
	"akka.remote.UniqueAddress" = akka-misc
}


#//#persistence-read-side
//lagom.persistence.read-side {
//
//	# how long should we wait when retrieving the last known offset
//	offset-timeout = 5s
//
//	# Exponential backoff for failures in ReadSideProcessor
//	failure-exponential-backoff {
//		# minimum (initial) duration until processor is started again
//		# after failure
//		min = 3s
//
//		# the exponential back-off is capped to this duration
//		max = 30s
//
//		# additional random delay is based on this factor
//		random-factor = 0.2
//	}
//
//	# The amount of time that a node should wait for the global prepare callback to execute
//	global-prepare-timeout = 40s
//
//	# Specifies that the read side processors should run on cluster nodes with a specific role.
//	# If the role is not specified (or empty) all nodes in the cluster are used.
//	run-on-role = ""
//
//	# The Akka dispatcher to use for read-side actors and tasks.
//	use-dispatcher = "lagom.persistence.dispatcher"
//}
#//#persistence-read-side

akka.kafka.consumer {
	poll-interval = 1s
}
akka.kafka.committer {
	max-interval = 1s
}